# Digest Workflow

Main workflow for daily content aggregation.

## Trigger

- "daily digest"
- "check feeds"
- "what's new"
- "run feed reader"

## Workflow Steps

### Step 1: Load Feeds

```bash
# Read user's feeds file
bun run ~/.claude/skills/FeedReader/Tools/FeedManager.ts list
```

Parse the YAML to get list of URLs with metadata.

### Step 2: Load Cache

```bash
# Load existing content hashes
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts load
```

Returns JSON with previous content hashes for change detection.

### Step 3: Scrape Each URL

**Processing order:**
1. YouTube feeds first (fastest, RSS-based)
2. Blog/Newsletter feeds (BrightData)
3. Twitter feeds last (slowest, requires delays)

Route based on feed category:

#### YouTube Feeds (category: youtube)

```bash
# Extract channel ID from URL
bun run ~/.claude/skills/feed-reader/Tools/YouTubeHandler.ts extract-id "[YOUTUBE_URL]"

# Fetch channel RSS feed
bun run ~/.claude/skills/feed-reader/Tools/YouTubeHandler.ts fetch [CHANNEL_ID]

# Optionally extract transcript for new videos
bun run ~/.claude/skills/feed-reader/Tools/YouTubeHandler.ts transcript [VIDEO_ID]
```

Returns JSON with video list and transcripts.

#### Twitter/X Feeds (category: twitter)

```bash
# Extract handle from URL (e.g., @karpathy from twitter.com/karpathy)
# Scrape tweets from user timeline
bun run ~/.claude/skills/feed-reader/Tools/TwitterScraper.ts scrape [HANDLE]
```

**Rate limiting:** 10s delay between Twitter profiles, max 5 per batch.

#### Blog/Newsletter Feeds (category: blog, newsletter)

Use existing BrightData FourTierScrape workflow:

```
Use BrightData skill to scrape [URL] with FourTierScrape workflow
```

**Priority:** High priority feeds first, medium second, low if time permits.

### Step 4: Detect Changes

For each scraped content:

```bash
# Compare content hash with cache
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts check "[URL]" "[CONTENT_HASH]"
```

Returns: `new` | `unchanged` | `updated`

### Step 5: Filter New Content

Collect only feeds with status `new` or `updated`.

If no new content found:
```
No new content found from your feeds today.
Last checked: [timestamp]
```
Stop workflow here.

### Step 6: Synthesize Insights

For each feed with new content, use **Research ExtractKnowledge** workflow:

```
Use Research skill ExtractKnowledge workflow for this content:
[SCRAPED_CONTENT]
```

Collect key insights from each source.

### Step 7: Generate Digest

Create markdown digest with this structure:

```markdown
# Daily Digest - YYYY-MM-DD

## Summary

Checked **X** feeds. Found **Y** with new content.

## Highlights

Top 3-5 most interesting insights across all sources:
- [Source]: Key insight...
- [Source]: Another insight...

## By Category

### YouTube (N new videos)

#### [Channel Name]
**New video:** [Video Title]
> Transcript summary (first 200 words or key points from transcript)...

üì∫ [Watch on YouTube](video_link) | Published: [publish_date]

---

### Twitter/X (N new)

#### @username
> Latest tweet content or thread summary...

üìä 12 likes ¬∑ 3 retweets ¬∑ 2 replies
üîó [View on Twitter](tweet_link)

---

### Blogs (N new)

#### [Blog Name]
**New post:** [Title if available]
> Key insight extracted from the post...

[Link to original]

---

### Newsletters (N new)

#### [Newsletter Name]
**Edition:** [Title if available]
> Key insight extracted from the newsletter...

[Link to original]

---

## All Sources Checked

| Source | Category | Status | Last Updated |
|--------|----------|--------|--------------|
| [Name] | youtube | new | 2026-01-25 |
| [Name] | blog | new | 2026-01-25 |
| [Name] | twitter | unchanged | 2026-01-24 |
| [Name] | newsletter | unchanged | 2026-01-24 |

---
*Generated by FeedReader skill at [timestamp]*
```

### Step 8: Save Digest

Save to project directory: `.claude/History/digests/YYYY-MM-DD_digest.md`

```bash
# Create directory if needed (in project, not user home)
mkdir -p .claude/History/digests

# Save digest
# (Use Write tool to save the generated markdown to project directory)
```

**Note:** Always save to project directory (`$CWD/.claude/History/digests/`) not user home directory.

### Step 9: Update Cache

```bash
# Save new hashes for all checked URLs
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts save "[JSON_DATA]"
```

### Step 10: Report

Output summary to user:

```
Daily digest complete!

Checked: X feeds
New content: Y feeds
Digest saved to: ~/.claude/History/digests/YYYY-MM-DD_digest.md

Top highlights:
1. [Source] - [Key insight preview]
2. [Source] - [Key insight preview]
3. [Source] - [Key insight preview]
```

## Error Handling

### Category-Specific Failures

**YouTube Fetch Failure:**
- Log error: `Failed to fetch YouTube channel: [channel_name] - [error_reason]`
- Continue with remaining feeds
- Note in digest: "‚ö†Ô∏è Could not fetch YouTube channel: [Channel Name]"

**Twitter Scrape Failure:**
- Log error: `Failed to scrape Twitter: @[handle] - [error_reason]`
- Continue with remaining feeds
- Note in digest: "‚ö†Ô∏è Could not fetch @[handle] (Twitter may be rate limiting or blocking)"

**BrightData Scraping Failure:**
- Log error: `BrightData failed for URL: [url] - [reason]`
- Continue with remaining URLs
- Note in digest: "‚ö†Ô∏è Failed to fetch: [URL] - [reason]"

**Critical:** Single feed failure must NOT block entire digest generation.

### No Feeds Configured

If feeds.yaml is empty or missing:
```
No feeds configured. Add feeds first:
  "add feed https://example.com/blog"
```

### All Scrapes Failed

If no URLs could be scraped:
```
Failed to fetch any feeds. Check:
1. Internet connection
2. Handler/scraper installations
3. URL validity
4. Rate limiting issues
```

## Performance Notes

- **100 URLs**: ~10-15 minutes with caching
- **Parallel scraping**: Process up to 5 URLs concurrently
- **Cache optimization**: Skip unchanged content (saves ~70% time on repeat runs)
- **Rate limiting**: 2-second delay between requests to same domain
