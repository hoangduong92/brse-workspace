# Digest Workflow

Main workflow for daily content aggregation.

## Trigger

- "daily digest"
- "check feeds"
- "what's new"
- "run feed reader"

## Workflow Steps

### Step 1: Load Feeds

```bash
# Read user's feeds file
bun run ~/.claude/skills/FeedReader/Tools/FeedManager.ts list
```

Parse the YAML to get list of URLs with metadata.

### Step 2: Load Cache

```bash
# Load existing content hashes
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts load
```

Returns JSON with previous content hashes for change detection.

### Step 3: Scrape Each URL

For each feed in the list, use **BrightData FourTierScrape** workflow:

```
Use BrightData skill to scrape [URL] with FourTierScrape workflow
```

**Processing order:**
1. High priority feeds first
2. Medium priority second
3. Low priority if time permits

**For Twitter/X URLs:** Expect Tier 3 (Browser) or Tier 4 (Bright Data API) to be used.

### Step 4: Detect Changes

For each scraped content:

```bash
# Compare content hash with cache
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts check "[URL]" "[CONTENT_HASH]"
```

Returns: `new` | `unchanged` | `updated`

### Step 5: Filter New Content

Collect only feeds with status `new` or `updated`.

If no new content found:
```
No new content found from your feeds today.
Last checked: [timestamp]
```
Stop workflow here.

### Step 6: Synthesize Insights

For each feed with new content, use **Research ExtractKnowledge** workflow:

```
Use Research skill ExtractKnowledge workflow for this content:
[SCRAPED_CONTENT]
```

Collect key insights from each source.

### Step 7: Generate Digest

Create markdown digest with this structure:

```markdown
# Daily Digest - YYYY-MM-DD

## Summary

Checked **X** feeds. Found **Y** with new content.

## Highlights

Top 3-5 most interesting insights across all sources:
- [Source]: Key insight...
- [Source]: Another insight...

## By Category

### Blogs (N new)

#### [Blog Name]
**New post:** [Title if available]
> Key insight extracted from the post...

[Link to original]

---

### Twitter/X (N new)

#### @username
> Summary of recent tweets/threads...

---

### Newsletters (N new)

...

---

## All Sources Checked

| Source | Category | Status | Last Updated |
|--------|----------|--------|--------------|
| [Name] | blog | new | 2026-01-25 |
| [Name] | twitter | unchanged | 2026-01-24 |

---
*Generated by FeedReader skill at [timestamp]*
```

### Step 8: Save Digest

Save to: `~/.claude/History/digests/YYYY-MM-DD_digest.md`

```bash
# Create directory if needed
mkdir -p ~/.claude/History/digests

# Save digest
# (Use Write tool to save the generated markdown)
```

### Step 9: Update Cache

```bash
# Save new hashes for all checked URLs
bun run ~/.claude/skills/FeedReader/Tools/ContentCache.ts save "[JSON_DATA]"
```

### Step 10: Report

Output summary to user:

```
Daily digest complete!

Checked: X feeds
New content: Y feeds
Digest saved to: ~/.claude/History/digests/YYYY-MM-DD_digest.md

Top highlights:
1. [Source] - [Key insight preview]
2. [Source] - [Key insight preview]
3. [Source] - [Key insight preview]
```

## Error Handling

### URL Scraping Failed

If BrightData fails for a URL after all 4 tiers:
- Log the failure
- Continue with remaining URLs
- Note in digest: "Failed to fetch: [URL] - [reason]"

### No Feeds Configured

If feeds.yaml is empty or missing:
```
No feeds configured. Add feeds first:
  "add feed https://example.com/blog"
```

### All Scrapes Failed

If no URLs could be scraped:
```
Failed to fetch any feeds. Check:
1. Internet connection
2. BrightData skill installation
3. URL validity
```

## Performance Notes

- **100 URLs**: ~10-15 minutes with caching
- **Parallel scraping**: Process up to 5 URLs concurrently
- **Cache optimization**: Skip unchanged content (saves ~70% time on repeat runs)
- **Rate limiting**: 2-second delay between requests to same domain
